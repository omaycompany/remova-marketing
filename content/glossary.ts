export interface GlossaryTerm {
    slug: string;
    term: string;
    metaTitle: string;
    metaDescription: string;
    definition: string;
    explanation: string;
    relatedTerms: string[];
    faqs?: { question: string; answer: string }[];
}

const t = (slug: string, term: string, definition: string, explanation: string, relatedTerms: string[]): GlossaryTerm => ({
    slug, term,
    metaTitle: `What is ${term}? | AI Glossary | Remova`,
    metaDescription: `${definition.slice(0, 150)}. Learn more about ${term} and how it applies to enterprise AI.`,
    definition, explanation, relatedTerms,
});

export const glossaryTerms: GlossaryTerm[] = [
    t("ai-guardrails", "AI Guardrails", "Safety mechanisms that constrain AI system behavior to prevent harmful, biased, or off-policy outputs.", "AI guardrails are control mechanisms implemented around AI systems to ensure their outputs remain within acceptable boundaries. They can operate at the input level (filtering prompts before they reach the model) or at the output level (screening responses before delivery to users). Modern enterprise guardrails typically include content filtering, topic restrictions, PII detection, and policy enforcement. Remova implements dual-layer guardrails combining instant rule-based matching with AI-powered semantic analysis for comprehensive protection.", ["semantic-filtering", "prompt-injection", "content-safety", "ai-safety-layer"]),
    t("pii-redaction", "PII Redaction", "The automatic detection and removal of personally identifiable information from text before it reaches AI models.", "PII redaction in the context of enterprise AI involves scanning user prompts for sensitive data patterns — Social Security numbers, credit card numbers, email addresses, phone numbers, names, and other identifiers — and either blocking the prompt entirely or replacing the PII with anonymized tokens before forwarding to the AI provider. This prevents sensitive data from being transmitted to third-party AI services, supporting GDPR, HIPAA, and CCPA compliance.", ["dlp", "data-sovereignty", "zero-retention", "ai-safety-layer"]),
    t("llm", "Large Language Model (LLM)", "A deep learning model trained on vast text datasets that can understand and generate human-like text.", "Large Language Models are neural networks with billions of parameters trained on internet-scale text data. They power modern AI assistants like ChatGPT (GPT-4), Claude, and Gemini. LLMs can perform tasks including text generation, summarization, translation, coding, and reasoning. Enterprise deployment of LLMs requires governance controls to manage costs, protect data, and ensure appropriate usage.", ["foundation-model", "token", "context-window", "inference-cost"]),
    t("prompt-injection", "Prompt Injection", "An attack technique where malicious instructions are embedded in user prompts to manipulate AI model behavior.", "Prompt injection attacks attempt to override an AI system's instructions by embedding hidden commands within seemingly normal prompts. Attackers may try to extract system prompts, bypass safety controls, or make the AI perform unintended actions. Defense requires multi-layered approaches including input sanitization, semantic analysis, and output verification.", ["jailbreaking", "ai-guardrails", "semantic-filtering", "red-teaming"]),
    t("ai-governance", "AI Governance", "The framework of policies, processes, and controls that guide responsible AI development and usage within organizations.", "AI governance encompasses the rules, responsibilities, and mechanisms organizations use to ensure AI systems are developed and used responsibly. This includes defining acceptable use policies, implementing technical controls, establishing oversight committees, and maintaining compliance with regulations like the EU AI Act and NIST AI RMF.", ["ai-ethics", "responsible-ai", "compliance-framework", "ai-audit"]),
    t("data-sovereignty", "Data Sovereignty", "The principle that data is subject to the laws and governance of the jurisdiction where it is collected or processed.", "Data sovereignty requires that data remains within specific geographic boundaries and complies with local regulations. For enterprise AI, this means ensuring AI queries and responses are processed in approved regions, model providers meet local data handling requirements, and cross-border data flows comply with regulations like GDPR, LGPD, and PDPA.", ["ai-governance", "zero-retention", "dlp", "compliance-framework"]),
    t("model-routing", "Model Routing", "The automated process of directing AI queries to the optimal model based on cost, latency, capability, or policy requirements.", "Intelligent model routing analyzes incoming queries and routes them to the most appropriate AI model based on configurable criteria. Simple tasks might be routed to cheaper, faster models, while complex reasoning tasks go to premium models. Privacy-sensitive queries can be directed to models with stronger data protection guarantees.", ["ai-gateway", "model-orchestration", "inference-cost", "model-endpoint"]),
    t("ai-finops", "AI FinOps", "The practice of managing and optimizing AI and LLM costs through financial governance, budgeting, and usage analytics.", "AI FinOps applies financial operations principles to AI spending. It includes setting department-level budgets, tracking per-query costs across multiple AI providers, normalizing costs through universal credit systems, and providing dashboards for cost analysis and optimization. As enterprise AI spending grows, FinOps becomes critical for sustainable adoption.", ["credit-system", "inference-cost", "token", "ai-budget"]),
    t("rbac", "Role-Based Access Control (RBAC)", "A security model that restricts system access based on organizational roles assigned to users.", "RBAC in enterprise AI platforms controls what actions users can perform based on their role — typically Organization Admin, Department Head, or Standard User. Admins configure global policies, department heads manage their teams, and users operate within assigned boundaries. RBAC ensures least-privilege access and supports compliance requirements.", ["multi-tenancy", "ai-governance", "ai-audit", "sso"]),
    t("dlp", "Data Loss Prevention (DLP)", "Technologies and practices that detect and prevent unauthorized transmission of sensitive data.", "DLP for AI involves scanning all interactions between users and AI models for sensitive data patterns. This includes detecting PII, financial data, API keys, passwords, source code, and proprietary information. When sensitive data is detected, DLP systems can block the transmission, redact the sensitive portions, or alert security teams.", ["pii-redaction", "ai-guardrails", "content-safety", "ai-safety-layer"]),
    t("rag", "Retrieval-Augmented Generation (RAG)", "A technique that grounds AI responses in retrieved documents to improve accuracy and reduce hallucinations.", "RAG combines information retrieval with text generation. When a user asks a question, the system first searches a knowledge base of uploaded documents to find relevant information, then provides this context to the AI model along with the user's query. This grounds the AI's response in factual, organization-specific data rather than relying solely on the model's training data.", ["vector-database", "knowledge-graph", "embedding", "ai-hallucination"]),
    t("semantic-filtering", "Semantic Filtering", "AI-powered content analysis that understands meaning and intent rather than relying on keyword matching.", "Semantic filtering goes beyond simple keyword blacklists to understand the meaning behind text. It can detect paraphrased sensitive content, recognize intent behind seemingly innocent queries, and identify policy violations that wouldn't trigger keyword-based rules. This makes it effective against sophisticated attempts to bypass safety controls.", ["ai-guardrails", "prompt-injection", "content-safety", "dlp"]),
    t("jailbreaking", "Jailbreaking (AI)", "Techniques used to bypass AI safety controls and make models produce restricted or harmful outputs.", "AI jailbreaking involves crafting prompts that trick AI models into ignoring their safety training and producing outputs they're designed to refuse. Techniques include role-playing scenarios, encoding instructions, and multi-step manipulation. Enterprise guardrails must detect and prevent these attempts to maintain safety standards.", ["prompt-injection", "red-teaming", "ai-guardrails", "adversarial-attack"]),
    t("token", "Token", "The basic unit of text processing in LLMs — typically a word, subword, or character that models use for input and output.", "Tokens are the fundamental units that LLMs process. A token can be a complete word, part of a word, or a punctuation mark. GPT models use roughly 1 token per 0.75 words. Token counts directly impact costs (providers charge per token) and capabilities (context windows are measured in tokens). Understanding tokenization is essential for AI cost management.", ["inference-cost", "context-window", "llm", "ai-finops"]),
    t("ai-hallucination", "AI Hallucination", "When an AI model generates factually incorrect information presented as truth.", "AI hallucinations occur when LLMs confidently produce information that is fabricated, inaccurate, or nonsensical. This is particularly dangerous in enterprise settings where AI-generated content may be used for decision-making, client communications, or regulatory filings. RAG and output verification help mitigate hallucination risks.", ["rag", "ai-guardrails", "responsible-ai", "semantic-filtering"]),
    t("shadow-ai", "Shadow AI", "Unauthorized use of AI tools by employees outside of IT-approved channels.", "Shadow AI refers to employees using personal ChatGPT, Claude, or other AI accounts for work purposes without organizational oversight. This creates significant security risks as sensitive company data may be uploaded to consumer AI services with no governance, auditing, or data protection controls. The solution is providing governed AI access that's superior to personal alternatives.", ["ai-governance", "dlp", "ai-guardrails", "rbac"]),
    t("ai-audit", "AI Audit", "A systematic examination of AI system operations, decisions, and impacts for compliance and quality assurance.", "AI audits review how AI systems are being used within an organization, what data they process, what outputs they generate, and whether usage complies with internal policies and external regulations. Audit trails should be immutable, comprehensive, and readily available for regulatory examination.", ["ai-governance", "compliance-framework", "responsible-ai", "ai-ethics"]),
    t("compliance-framework", "Compliance Framework", "A structured set of guidelines and controls that ensure AI systems meet regulatory and organizational requirements.", "AI compliance frameworks define the policies, procedures, and technical controls needed to meet regulatory requirements. Key frameworks include the EU AI Act, NIST AI Risk Management Framework, ISO 42001, SOC 2, GDPR, and HIPAA. Organizations must map their AI usage to applicable frameworks and implement appropriate controls.", ["ai-governance", "ai-audit", "responsible-ai", "ai-ethics"]),
    t("ai-risk", "AI Risk", "Potential negative outcomes from AI system deployment, including data leaks, bias, hallucinations, and security vulnerabilities.", "AI risks span multiple categories: security risks (data leaks, prompt injection), operational risks (hallucinations, model failures), compliance risks (regulatory violations), reputational risks (biased or inappropriate outputs), and financial risks (uncontrolled costs). Effective AI risk management requires proactive controls rather than reactive responses.", ["ai-governance", "ai-guardrails", "responsible-ai", "ai-audit"]),
    t("on-premises-ai", "On-Premises AI", "AI deployment where all infrastructure, models, and data processing occur within an organization's own facilities.", "On-premises AI deployment keeps all components — the governance layer, AI models, and data — within the organization's controlled infrastructure. This provides maximum data security, supports air-gapped environments, and ensures compliance with the strictest data residency requirements. Organizations can run open-source models like Llama and Mistral locally.", ["data-sovereignty", "zero-retention", "ai-governance", "federated-learning"]),
    t("zero-retention", "Zero-Retention Policy", "An architecture where no user conversation data is stored on servers, only existing in the user's browser.", "Zero-retention (or zero-history) architecture means the platform stores absolutely no conversation data on its servers. All chat history exists exclusively in the user's browser using local storage. This provides privacy by architecture rather than policy — there's simply no server-side data to breach, subpoena, or accidentally expose.", ["data-sovereignty", "pii-redaction", "on-premises-ai", "ai-governance"]),
    t("multi-tenancy", "Multi-Tenancy", "An architecture where a single platform instance serves multiple isolated organizational units or customers.", "Multi-tenancy in enterprise AI allows a single deployment to serve multiple departments, teams, or subsidiaries with strict data and cost isolation between them. Each tenant gets independent budgets, policies, model access, and audit logs with zero cross-contamination.", ["rbac", "ai-governance", "department-management", "ai-finops"]),
    t("ai-safety-layer", "AI Safety Layer", "A middleware component that sits between users and AI models to enforce safety policies and controls.", "An AI safety layer is an intermediary system that intercepts, analyzes, and potentially modifies all communications between users and AI models. It enforces organizational policies, blocks sensitive data, prevents prompt injection, and ensures outputs comply with brand and safety guidelines. Remova functions as a comprehensive AI safety layer.", ["ai-guardrails", "semantic-filtering", "dlp", "pii-redaction"]),
    t("foundation-model", "Foundation Model", "A large AI model trained on broad data that can be adapted to many downstream tasks.", "Foundation models are large-scale AI models (like GPT-4, Claude, Gemini) trained on vast datasets that serve as the base for various applications. They can perform multiple tasks without task-specific training. Enterprise governance must account for the fact that foundation models may contain biases from training data and can generate unpredictable outputs.", ["llm", "fine-tuning", "training-data", "ai-alignment"]),
    t("fine-tuning", "Fine-Tuning", "The process of further training a pre-trained AI model on specific data to customize its behavior for particular tasks.", "Fine-tuning adapts a foundation model's capabilities to specific organizational needs by training it on domain-specific data. This can improve accuracy for specialized tasks, align outputs with organizational tone and style, and reduce hallucinations in domain-specific contexts.", ["foundation-model", "training-data", "llm", "system-prompt"]),
    t("system-prompt", "System Prompt", "Hidden instructions given to an AI model that define its behavior, personality, and constraints for a conversation.", "System prompts are instructions provided to AI models before user interaction begins. They define the AI's role, behavioral guidelines, topic restrictions, and output formatting. In enterprise settings, system prompts are used to create specialized AI assistants, enforce brand voice, and provide business context.", ["ai-guardrails", "fine-tuning", "llm", "content-safety"]),
    t("context-window", "Context Window", "The maximum amount of text (measured in tokens) that an AI model can process in a single conversation.", "The context window determines how much text an AI model can consider at once — including the system prompt, conversation history, and user input. Larger context windows (e.g., Claude's 200K tokens) allow processing longer documents but increase costs. Managing context efficiently is important for both quality and cost optimization.", ["token", "llm", "inference-cost", "rag"]),
    t("ai-gateway", "AI Gateway", "A centralized access point that manages, monitors, and controls traffic between applications and AI model providers.", "An AI gateway sits between your organization and AI model providers, providing a single point of control for authentication, rate limiting, cost tracking, security scanning, and policy enforcement. It enables vendor-agnostic model access and simplifies multi-provider management.", ["model-routing", "model-orchestration", "ai-finops", "api-management"]),
    t("model-orchestration", "Model Orchestration", "The coordination of multiple AI models to work together on complex tasks or provide redundancy.", "Model orchestration manages the selection, sequencing, and coordination of multiple AI models. This includes choosing the right model for each task, managing failover between providers, combining outputs from multiple models, and balancing cost and quality across model selections.", ["model-routing", "ai-gateway", "model-endpoint", "inference-cost"]),
    t("ai-budget", "AI Budget", "A defined spending limit for AI usage, typically allocated per department, team, or individual user.", "AI budgets set financial guardrails around AI usage to prevent uncontrolled spending. They can include hard caps (which stop usage when reached), soft alerts (which notify administrators), and auto-renewal logic. Effective AI budgeting normalizes costs across different providers and models using universal credit systems.", ["ai-finops", "credit-system", "inference-cost", "token"]),
    t("credit-system", "Credit System", "A universal currency that normalizes AI costs across multiple providers into a single internal unit.", "A credit system creates an internal currency that represents AI usage value across different providers and models. One credit might equal a certain number of GPT-4 tokens or Claude tokens, normalized for comparable value. This simplifies budgeting, cost comparison, and cross-provider usage tracking.", ["ai-finops", "ai-budget", "inference-cost", "token"]),
    t("sso", "Single Sign-On (SSO)", "An authentication method that allows users to access multiple applications with one set of credentials.", "SSO for enterprise AI platforms enables employees to access AI tools using their existing corporate credentials (Okta, Azure AD, Google Workspace). This eliminates password fatigue, simplifies user provisioning, ensures access revocation on termination, and integrates with existing identity management workflows.", ["rbac", "multi-tenancy", "ai-governance", "department-management"]),
    t("ai-ethics", "AI Ethics", "The principles and guidelines governing the responsible development and use of AI systems.", "AI ethics addresses questions of fairness, transparency, accountability, and harm prevention in AI systems. For enterprises, ethical AI use means ensuring AI doesn't discriminate, respecting user privacy, being transparent about AI usage, and maintaining human oversight over important decisions.", ["responsible-ai", "ai-governance", "ai-bias", "explainability"]),
    t("responsible-ai", "Responsible AI", "An approach to AI development and deployment that prioritizes safety, fairness, transparency, and accountability.", "Responsible AI is the practical implementation of AI ethics principles. It includes bias testing, explainability, human oversight, privacy protection, and environmental consideration. Organizations implementing responsible AI programs need technical controls (guardrails, audits) alongside policy frameworks.", ["ai-ethics", "ai-governance", "ai-bias", "explainability"]),
    t("ai-bias", "AI Bias", "Systematic errors in AI outputs that result from biased training data or flawed model design.", "AI bias occurs when models produce outputs that systematically favor or disadvantage certain groups. Sources include biased training data, biased labeling, and algorithmic design choices. Enterprise AI governance must include bias detection and mitigation, particularly for decisions affecting people (hiring, lending, healthcare).", ["ai-ethics", "responsible-ai", "explainability", "ai-audit"]),
    t("explainability", "Explainability (XAI)", "The ability to understand and explain how an AI model arrives at its outputs or decisions.", "Explainable AI (XAI) provides transparency into model decision-making. In enterprise settings, explainability is important for regulatory compliance (especially in finance and healthcare), building user trust, debugging model behavior, and satisfying audit requirements.", ["ai-ethics", "responsible-ai", "ai-audit", "ai-transparency"]),
    t("ai-transparency", "AI Transparency", "The practice of being open about how AI systems work, what data they use, and how decisions are made.", "AI transparency involves disclosing AI system capabilities and limitations, data sources and handling practices, decision-making processes, and known biases or risks. Transparency builds trust, supports compliance, and enables informed decision-making about AI adoption.", ["explainability", "ai-ethics", "responsible-ai", "ai-governance"]),
    t("data-leak-prevention", "Data Leak Prevention (for AI)", "Specific controls preventing sensitive organizational data from being exposed through AI interactions.", "Data leak prevention for AI goes beyond traditional DLP by addressing AI-specific vectors: users pasting sensitive documents into chat, uploading confidential files for analysis, or inadvertently sharing proprietary information through prompts. Dual-layer guardrails with PII redaction provide comprehensive protection.", ["dlp", "pii-redaction", "ai-guardrails", "semantic-filtering"]),
    t("content-safety", "Content Safety", "Mechanisms ensuring AI-generated content is appropriate, accurate, and aligned with organizational standards.", "Content safety for enterprise AI covers blocking inappropriate, harmful, or off-brand AI responses. This includes profanity filtering, misinformation detection, brand guideline enforcement, competitor mention prevention, and legal liability avoidance. Both input filtering and output verification are needed.", ["ai-guardrails", "semantic-filtering", "brand-safety", "ai-safety-layer"]),
    t("brand-safety", "Brand Safety (AI)", "Controls ensuring AI outputs align with organizational brand voice, values, and communication guidelines.", "Brand safety in AI prevents the generation of content that could damage brand reputation — including competitor endorsements, off-tone messaging, controversial opinions, and factually incorrect claims. Brand safety guardrails are configured per organization and can vary by department.", ["content-safety", "ai-guardrails", "system-prompt", "semantic-filtering"]),
    t("ai-policy", "AI Policy", "Organizational rules defining acceptable AI usage, data handling, and governance requirements.", "AI policies establish the ground rules for how employees may use AI tools. They cover acceptable use cases, prohibited activities, data handling requirements, approval processes, and consequences for violations. Technical enforcement through guardrails turns written policies into automated controls.", ["ai-governance", "compliance-framework", "ai-audit", "ai-ethics"]),
    t("model-card", "Model Card", "A documentation framework providing transparency about an AI model's capabilities, limitations, and intended use.", "Model cards describe a model's performance characteristics, training data, intended use cases, known limitations, and ethical considerations. They help organizations make informed decisions about which models to deploy and how to use them responsibly.", ["ai-transparency", "responsible-ai", "ai-governance", "foundation-model"]),
    t("ai-incident", "AI Incident", "An event where an AI system causes harm, produces incorrect outputs, or violates organizational policies.", "AI incidents include data leaks through AI, generation of harmful content, biased decisions affecting people, system outages, and policy violations. Enterprise AI governance should include incident detection, response procedures, documentation, and prevention through guardrails.", ["ai-risk", "ai-audit", "ai-guardrails", "ai-governance"]),
    t("red-teaming", "Red Teaming (AI)", "The practice of adversarially testing AI systems to discover vulnerabilities and failure modes.", "AI red teaming involves deliberately trying to break AI systems through jailbreaking, prompt injection, data extraction, and other attack techniques. The goal is to identify vulnerabilities before malicious actors do. Results inform guardrail configuration and security improvements.", ["jailbreaking", "prompt-injection", "adversarial-attack", "ai-guardrails"]),
    t("adversarial-attack", "Adversarial Attack (AI)", "Deliberate attempts to manipulate AI system behavior through crafted inputs.", "Adversarial attacks on AI systems include prompt injection, jailbreaking, data poisoning, and model extraction. In enterprise settings, adversarial attacks may come from employees trying to bypass guardrails or external actors targeting AI-powered services. Defense requires multiple layers of protection.", ["red-teaming", "prompt-injection", "jailbreaking", "ai-guardrails"]),
    t("embedding", "Embedding", "A numerical vector representation of text that captures semantic meaning for AI processing.", "Embeddings convert text into dense numerical vectors that capture semantic relationships. Similar concepts produce similar vectors. Embeddings are used for semantic search, RAG document retrieval, content similarity analysis, and classification tasks. They're fundamental to modern AI knowledge base systems.", ["vector-database", "rag", "knowledge-graph", "llm"]),
    t("vector-database", "Vector Database", "A database optimized for storing and querying high-dimensional vector embeddings for AI applications.", "Vector databases store embedding vectors and enable fast similarity searches — finding the most relevant documents for a given query. They're essential infrastructure for RAG systems, enabling AI to retrieve relevant organizational knowledge to ground its responses.", ["embedding", "rag", "knowledge-graph", "llm"]),
    t("knowledge-graph", "Knowledge Graph", "A structured representation of relationships between entities used to enhance AI reasoning.", "Knowledge graphs organize information into entities and relationships, enabling AI systems to reason about connections between concepts. In enterprise settings, knowledge graphs can represent organizational structure, product relationships, customer networks, and domain expertise.", ["rag", "embedding", "vector-database", "llm"]),
    t("ai-orchestration", "AI Orchestration", "The coordination of multiple AI services, tools, and workflows into cohesive automated processes.", "AI orchestration manages complex workflows involving multiple AI models and tools. This includes chaining AI calls, managing data flow between services, handling errors and retries, and coordinating human-in-the-loop steps. Enterprise orchestration requires governance at every step.", ["model-orchestration", "ai-agent", "model-routing", "ai-gateway"]),
    t("model-endpoint", "Model Endpoint", "An API URL where AI model inference requests are sent and responses received.", "Model endpoints are the technical access points for AI models. Each provider (OpenAI, Anthropic, Google) exposes endpoints for their models. Enterprise AI platforms abstract multiple endpoints behind a single gateway, managing authentication, routing, and failover transparently.", ["ai-gateway", "model-routing", "inference-cost", "model-orchestration"]),
    t("inference-cost", "Inference Cost", "The computational cost of running a query through an AI model, typically measured per token.", "Inference costs vary significantly across models: GPT-4o might cost $5 per million input tokens, while lighter models cost a fraction. Understanding inference costs is essential for AI budgeting, model selection, and cost optimization. Intelligent routing can reduce costs by directing simple tasks to cheaper models.", ["token", "ai-finops", "ai-budget", "model-routing"]),
    t("training-data", "Training Data", "The dataset used to train an AI model, which significantly influences its capabilities and biases.", "Training data determines what an AI model knows, how it behaves, and what biases it may exhibit. Understanding training data composition is important for assessing model suitability, predicting biases, and ensuring compliance. Enterprise governance should consider training data provenance when selecting models.", ["foundation-model", "ai-bias", "fine-tuning", "synthetic-data"]),
    t("synthetic-data", "Synthetic Data", "Artificially generated data that mimics real-world data characteristics without containing actual sensitive information.", "Synthetic data is created algorithmically to have the same statistical properties as real data without containing actual personal or sensitive information. It's used for AI training, testing, and development when real data is too sensitive, scarce, or expensive to use.", ["training-data", "pii-redaction", "ai-ethics", "foundation-model"]),
    t("ai-alignment", "AI Alignment", "Ensuring AI systems behave according to human values, intentions, and organizational goals.", "AI alignment is the challenge of making AI systems do what we actually want them to do. In enterprise settings, alignment means ensuring AI outputs match organizational values, follow brand guidelines, respect policies, and serve user intentions accurately. Guardrails and system prompts are practical alignment tools.", ["ai-ethics", "responsible-ai", "ai-guardrails", "system-prompt"]),
    t("reinforcement-learning", "Reinforcement Learning", "A training technique where AI learns optimal behavior through trial, error, and reward signals.", "Reinforcement Learning from Human Feedback (RLHF) is used to align LLMs with human preferences and safety requirements during training. Understanding RLHF helps explain why models behave as they do and why additional guardrails are needed for enterprise-specific policies.", ["ai-alignment", "foundation-model", "fine-tuning", "training-data"]),
    t("ai-agent", "AI Agent", "An AI system that can autonomously plan, reason, and take actions to accomplish multi-step tasks.", "AI agents go beyond simple question-answering to perform complex, multi-step tasks autonomously — browsing the web, writing code, managing files, and interacting with APIs. Enterprise governance for agents is especially critical because they can take real-world actions with potentially significant consequences.", ["ai-orchestration", "model-orchestration", "ai-guardrails", "ai-risk"]),
    t("multimodal-ai", "Multimodal AI", "AI systems that can process and generate multiple types of data including text, images, audio, and video.", "Multimodal AI models like GPT-4o and Gemini can understand and generate text, images, audio, and video. Enterprise governance must extend to all modalities — ensuring image generation follows brand guidelines, audio processing respects privacy, and video analysis complies with consent requirements.", ["llm", "foundation-model", "content-safety", "ai-guardrails"]),
    t("enterprise-ai", "Enterprise AI", "The strategic deployment of AI systems within large organizations with appropriate governance and security.", "Enterprise AI encompasses the tools, platforms, and governance frameworks organizations use to deploy AI at scale. It goes beyond individual AI tool usage to include centralized management, security controls, cost governance, compliance, and organizational alignment.", ["ai-governance", "ai-finops", "rbac", "multi-tenancy"]),
    t("ai-deployment", "AI Deployment", "The process of making AI models available for use in production environments with appropriate controls.", "AI deployment in enterprise settings involves selecting models, configuring access controls, setting up guardrails, establishing budgets, training users, and monitoring usage. Deployment can be cloud-hosted, on-premises, or hybrid depending on security requirements.", ["on-premises-ai", "enterprise-ai", "ai-governance", "model-endpoint"]),
    t("federated-learning", "Federated Learning", "A machine learning approach where models are trained across multiple devices without sharing raw data.", "Federated learning allows AI models to learn from distributed data sources without centralizing the data. This preserves privacy and data sovereignty while still benefiting from diverse datasets. It's particularly relevant for healthcare, finance, and cross-border collaborations.", ["on-premises-ai", "data-sovereignty", "training-data", "ai-ethics"]),
    t("department-management", "Department Management", "Organizational hierarchies within AI platforms that mirror company structure for access and budget control.", "Department management in enterprise AI platforms allows organizations to create hierarchical structures that mirror their org chart. Each department can have independent AI budgets, model access policies, guardrail configurations, and administrative oversight.", ["rbac", "multi-tenancy", "ai-finops", "enterprise-ai"]),
    t("api-management", "API Management (for AI)", "Tools and practices for managing, securing, and monitoring AI model API access and usage.", "AI API management involves controlling access to AI model endpoints, managing API keys, monitoring usage, enforcing rate limits, and tracking costs. Enterprise AI gateways provide centralized API management across multiple AI providers.", ["ai-gateway", "model-endpoint", "ai-finops", "rbac"]),
];
